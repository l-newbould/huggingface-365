{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a06aa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kdnuggets.com/using-hugging-face-transformers-with-pytorch-and-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "562753e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "927f0b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df858fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = load_dataset(\"asuender/motivational-quotes\", \"quotes\", split=\"train\")\n",
    "dataset = load_dataset(\"SetFit/bbc-news\", split = \"train\")\n",
    "#dataset = load_dataset(\"imdb\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d23f837a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'wales want rugby league training wales could follow england s lead by training with a rugby league club.  england have already had a three-day session with leeds rhinos  and wales are thought to be interested in a similar clinic with rivals st helens. saints coach ian millward has given his approval  but if it does happen it is unlikely to be this season. saints have a week s training in portugal next week  while wales will play england in the opening six nations match on 5 february.  we have had an approach from wales   confirmed a saints spokesman.  it s in the very early stages but it is something we are giving serious consideration to.  st helens  who are proud of their welsh connections  are obvious partners for the welsh rugby union  despite a spat in 2001 over the collapse of kieron cunningham s proposed Â£500 000 move to union side swansea. a similar cross-code deal that took iestyn harris from leeds to cardiff in 2001 did go through  before the talented stand-off returned to the 13-man code with bradford bulls. kel coslett  who famously moved from wales to league in the 1960s  is currently saints  football manager  while clive griffiths - wales  defensive coach - is a former st helens player and is thought to be the man behind the latest initiative. scott gibbs  the former wales and lions centre  played for st helens from 1994-96 and was in the challenge cup-winning team at wembley in 1996.',\n",
       " 'label': 2,\n",
       " 'label_text': 'sport'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc9191a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': None,\n",
       " 'format_kwargs': {},\n",
       " 'columns': ['text', 'label', 'label_text'],\n",
       " 'output_all_columns': False}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5fc1aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1: set the format of columns using with_format(), create PyTorch tensors by setting type=\"torch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df6cea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "py_1 = dataset.with_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3d5231e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'torch',\n",
       " 'format_kwargs': {},\n",
       " 'columns': ['text', 'label', 'label_text'],\n",
       " 'output_all_columns': False}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py_1.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "518d7cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_1 = dataset.with_format(type=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8cae3ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'tensorflow',\n",
       " 'format_kwargs': {},\n",
       " 'columns': ['text', 'label', 'label_text'],\n",
       " 'output_all_columns': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_1.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30a8c737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also set our tokenizer options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0643af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1503ee2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 3575, 2215, 4043, 2223, 2731, 3575, 2071, 3582, 2563, 1055, 2599,\n",
       "         2011, 2731, 2007, 1037, 4043, 2223, 2252,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(dataset[0]['text'], max_length=20, padding='max_length', truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c63c1fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 20), dtype=int32, numpy=\n",
       "array([[ 101, 3575, 2215, 4043, 2223, 2731, 3575, 2071, 3582, 2563, 1055,\n",
       "        2599, 2011, 2731, 2007, 1037, 4043, 2223, 2252,  102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 20), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 20), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])>}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(dataset[0]['text'], max_length=20, padding='max_length', truncation=True, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8bcc34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or apply transformations after tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c515434d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39ef519eb25b48e98823d7f5d2210343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1225 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], max_length=50, padding='max_length', truncation=True)\n",
    "\n",
    "tokenized = dataset.map(tokenize_function, batched=True)\n",
    "tokenized = tokenized.remove_columns(['text', 'label_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8e5c01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_py_1 = tokenized.with_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10696b6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': tensor(2),\n",
       " 'input_ids': tensor([  101,  3575,  2215,  4043,  2223,  2731,  3575,  2071,  3582,  2563,\n",
       "          1055,  2599,  2011,  2731,  2007,  1037,  4043,  2223,  2252,  1012,\n",
       "          2563,  2031,  2525,  2018,  1037,  2093,  1011,  2154,  5219,  2007,\n",
       "          7873, 24091,  2015,  1998,  3575,  2024,  2245,  2000,  2022,  4699,\n",
       "          1999,  1037,  2714,  9349,  2007,  9169,  2358, 24074,  1012,   102]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_py_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa4d0183",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_py_1 = tokenized_py_1.train_test_split(test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e47bbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now with tensorflow\n",
    "# pip install --no-deps tensorflow (running without no deps broke numpy)\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00b3bdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tf_1 = tokenized.with_format(type='tf', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5398447",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': <tf.Tensor: shape=(), dtype=int64, numpy=2>,\n",
       " 'input_ids': <tf.Tensor: shape=(50,), dtype=int64, numpy=\n",
       " array([  101,  3575,  2215,  4043,  2223,  2731,  3575,  2071,  3582,\n",
       "         2563,  1055,  2599,  2011,  2731,  2007,  1037,  4043,  2223,\n",
       "         2252,  1012,  2563,  2031,  2525,  2018,  1037,  2093,  1011,\n",
       "         2154,  5219,  2007,  7873, 24091,  2015,  1998,  3575,  2024,\n",
       "         2245,  2000,  2022,  4699,  1999,  1037,  2714,  9349,  2007,\n",
       "         9169,  2358, 24074,  1012,   102], dtype=int64)>,\n",
       " 'token_type_ids': <tf.Tensor: shape=(50,), dtype=int64, numpy=\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0], dtype=int64)>,\n",
       " 'attention_mask': <tf.Tensor: shape=(50,), dtype=int64, numpy=\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1], dtype=int64)>}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_tf_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "705ddc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tf_1 = tokenized_tf_1.train_test_split(test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e49b02de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\lnewb\\anaconda3\\envs\\huggingface_course_env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\lnewb\\anaconda3\\envs\\huggingface_course_env\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# loading model\n",
    "# pip install tf-keras\n",
    "\n",
    "#pytorch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model_pt = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=5, torch_dtype=\"auto\")\n",
    "\n",
    "#tensorflow\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "model_tf = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7566dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='54' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 3/54 00:06 < 05:44, 0.15 it/s, Epoch 0.07/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "# pytorch model\n",
    "# we can use the trainer for high level cutsomization \n",
    "#PyTorch\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./pt_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_pt,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_py_1['train'],\n",
    "    eval_dataset=tokenized_py_1['test'],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df34de2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6dd741f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "dictionary update sequence element #0 has length 3; 2 is required",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#TensorFlow\u001b[39;00m\n\u001b[0;32m      2\u001b[0m model_tf\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-5\u001b[39m),\n\u001b[0;32m      3\u001b[0m                  loss\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m      4\u001b[0m                  metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 6\u001b[0m model_tf\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtokenized_tf_1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: dictionary update sequence element #0 has length 3; 2 is required"
     ]
    }
   ],
   "source": [
    "#TensorFlow\n",
    "model_tf.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "                 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "model_tf.fit(tokenized_tf_1['train'], epochs=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface_course_env",
   "language": "python",
   "name": "huggingface_course_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
