{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e5d6193-3416-4ae8-b488-d052266bea79",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "927f0b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df858fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"SetFit/bbc-news\", split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d23f837a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'wales want rugby league training wales could follow england s lead by training with a rugby league club.  england have already had a three-day session with leeds rhinos  and wales are thought to be interested in a similar clinic with rivals st helens. saints coach ian millward has given his approval  but if it does happen it is unlikely to be this season. saints have a week s training in portugal next week  while wales will play england in the opening six nations match on 5 february.  we have had an approach from wales   confirmed a saints spokesman.  it s in the very early stages but it is something we are giving serious consideration to.  st helens  who are proud of their welsh connections  are obvious partners for the welsh rugby union  despite a spat in 2001 over the collapse of kieron cunningham s proposed Â£500 000 move to union side swansea. a similar cross-code deal that took iestyn harris from leeds to cardiff in 2001 did go through  before the talented stand-off returned to the 13-man code with bradford bulls. kel coslett  who famously moved from wales to league in the 1960s  is currently saints  football manager  while clive griffiths - wales  defensive coach - is a former st helens player and is thought to be the man behind the latest initiative. scott gibbs  the former wales and lions centre  played for st helens from 1994-96 and was in the challenge cup-winning team at wembley in 1996.',\n",
       " 'label': 2,\n",
       " 'label_text': 'sport'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc9191a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': None,\n",
       " 'format_kwargs': {},\n",
       " 'columns': ['text', 'label', 'label_text'],\n",
       " 'output_all_columns': False}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.format # note type = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6f8fb1-7ff1-4dfe-bfad-dd48d14c953a",
   "metadata": {},
   "source": [
    "## Convert dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9fd3b7-acf3-40d2-b337-0c41549813e0",
   "metadata": {},
   "source": [
    "### Option 1: format using with_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ce0881-97ee-4fd2-9c8b-c6670c1cee6f",
   "metadata": {},
   "source": [
    "**Pytorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df6cea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_format = dataset.with_format(type=\"torch\")\n",
    "pt_format.format # note note type = Torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0955fd43-9b51-4a6d-be34-62e0249bdd4d",
   "metadata": {},
   "source": [
    "**Tensorflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "518d7cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_format = dataset.with_format(type=\"tf\")\n",
    "tf_format.format # note note type = tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77bf23f-5c45-4a39-bb59-7f536cce9ef8",
   "metadata": {},
   "source": [
    "### Option 2: set our tokenizer to return the desired format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0643af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1711d6-6ff4-4db7-bc17-1916b097cc83",
   "metadata": {},
   "source": [
    "**Pytorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1503ee2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 3575, 2215, 4043, 2223, 2731, 3575, 2071, 3582, 2563, 1055, 2599,\n",
       "         2011, 2731, 2007, 1037, 4043, 2223, 2252,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(dataset[0]['text'], max_length=20, padding='max_length', truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e145ce5-4ac2-4dbd-baf0-c5336c5c8c5b",
   "metadata": {},
   "source": [
    "**Tensorflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c63c1fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 20), dtype=int32, numpy=\n",
       "array([[ 101, 3575, 2215, 4043, 2223, 2731, 3575, 2071, 3582, 2563, 1055,\n",
       "        2599, 2011, 2731, 2007, 1037, 4043, 2223, 2252,  102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 20), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 20), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])>}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(dataset[0]['text'], max_length=20, padding='max_length', truncation=True, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c6a590-1e8e-4e94-afed-14750c9002d8",
   "metadata": {},
   "source": [
    "### Option 3: apply transformations after tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c515434d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39ef519eb25b48e98823d7f5d2210343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1225 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize our dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "tokenized = dataset.map(tokenize_function, batched=True)\n",
    "tokenized = tokenized.remove_columns(['text', 'label_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091eca24-d156-4ea0-b1c7-32fbaca5ec5a",
   "metadata": {},
   "source": [
    "**Pytorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8e5c01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_pt = tokenized.with_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa4d0183",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_pt = tokenized_pt.train_test_split(test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10696b6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': tensor(2),\n",
       " 'input_ids': tensor([  101,  3575,  2215,  4043,  2223,  2731,  3575,  2071,  3582,  2563,\n",
       "          1055,  2599,  2011,  2731,  2007,  1037,  4043,  2223,  2252,  1012,\n",
       "          2563,  2031,  2525,  2018,  1037,  2093,  1011,  2154,  5219,  2007,\n",
       "          7873, 24091,  2015,  1998,  3575,  2024,  2245,  2000,  2022,  4699,\n",
       "          1999,  1037,  2714,  9349,  2007,  9169,  2358, 24074,  1012,   102]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_pt['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abac9b20-9973-4861-bde8-9f816460e8eb",
   "metadata": {},
   "source": [
    "**Tensorflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116df33b-1065-4edc-b0ab-1351e64766ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --no-deps tensorflow (running without no deps broke numpy)\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9e47bbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tf = tokenized.with_format(type='tf', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "705ddc5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_PrefetchDataset' object has no attribute 'train_test_split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[118], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokenized_tf \u001b[38;5;241m=\u001b[39m \u001b[43mtokenized_tf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_test_split\u001b[49m(test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_PrefetchDataset' object has no attribute 'train_test_split'"
     ]
    }
   ],
   "source": [
    "tokenized_tf = tokenized_tf.train_test_split(test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c5398447",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': <tf.Tensor: shape=(), dtype=int64, numpy=2>,\n",
       " 'input_ids': <tf.Tensor: shape=(50,), dtype=int64, numpy=\n",
       " array([  101,  3575,  2215,  4043,  2223,  2731,  3575,  2071,  3582,\n",
       "         2563,  1055,  2599,  2011,  2731,  2007,  1037,  4043,  2223,\n",
       "         2252,  1012,  2563,  2031,  2525,  2018,  1037,  2093,  1011,\n",
       "         2154,  5219,  2007,  7873, 24091,  2015,  1998,  3575,  2024,\n",
       "         2245,  2000,  2022,  4699,  1999,  1037,  2714,  9349,  2007,\n",
       "         9169,  2358, 24074,  1012,   102], dtype=int64)>,\n",
       " 'token_type_ids': <tf.Tensor: shape=(50,), dtype=int64, numpy=\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0], dtype=int64)>,\n",
       " 'attention_mask': <tf.Tensor: shape=(50,), dtype=int64, numpy=\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1], dtype=int64)>}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_tf['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50afd10-6ee2-4f3f-9138-17275fac34bd",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a9809d-a57d-4cd0-864c-79cf16a31fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tf-keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f0ce29-1877-4765-81cc-51adeb2fe983",
   "metadata": {},
   "source": [
    "**Pytorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e49b02de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\lnewb\\anaconda3\\envs\\huggingface_course_env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\lnewb\\anaconda3\\envs\\huggingface_course_env\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model_pt = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=5, torch_dtype=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e894d61b-3f1e-44b1-a0e5-5c58ebfae4ec",
   "metadata": {},
   "source": [
    "**Tensorflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c704c2be-42ef-4be2-8fb4-4e02532b4a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "model_tf = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687f7e83-e252-457d-a047-f568a34350ea",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de01a08a-eb3b-41b7-bdda-693aadf2e24f",
   "metadata": {},
   "source": [
    "**Pytorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468081ee-0e50-4e6e-885b-188a1ae13fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e7566dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='54' max='54' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [54/54 07:08, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.103610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.110506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=54, training_loss=0.05103096255549678, metrics={'train_runtime': 435.7465, 'train_samples_per_second': 3.933, 'train_steps_per_second': 0.124, 'total_flos': 44041454705400.0, 'train_loss': 0.05103096255549678, 'epoch': 2.0})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"pt_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_pt,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_py['train'],\n",
    "    eval_dataset=tokenized_py['test']\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d772679-898c-4bd7-833e-de836903118e",
   "metadata": {},
   "source": [
    "**Tensorflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "01643e7f-1686-4fdd-9d84-03e8e78ee485",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tf = tokenized_tf['train'].to_tf_dataset(\n",
    "    columns=['input_ids', 'attention_mask'],\n",
    "    label_cols=[\"label\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16\n",
    ") # you could also run this directly on tokenized without doing the other steps above in option 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6dd741f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:From C:\\Users\\lnewb\\anaconda3\\envs\\huggingface_course_env\\Lib\\site-packages\\tf_keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\lnewb\\anaconda3\\envs\\huggingface_course_env\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "54/54 [==============================] - 172s 3s/step - loss: 1.0367 - accuracy: 0.7305\n",
      "Epoch 2/2\n",
      "54/54 [==============================] - 150s 3s/step - loss: 0.2418 - accuracy: 0.9708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x1c2a23e6ed0>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tf.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "                 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "model_tf.fit(tokenized_tf, epochs=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface_course_env",
   "language": "python",
   "name": "huggingface_course_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
